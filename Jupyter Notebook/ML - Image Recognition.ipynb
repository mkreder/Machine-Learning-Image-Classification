{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "<br></br>\n",
    "Take me to the [code and Jupyter Notebook](https://github.com/AMoazeni/Machine-Learning-Image-Recognition/blob/master/Jupyter%20Notebook/ML%20-%20Image%20Recognition.ipynb) for Image Recognition!\n",
    "\n",
    "<br></br>\n",
    "This article explores a Machine Learning algorithm called Convolution Neural Network (CNN), it's a common Deep Learning technique used for image recognition and classification.\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Data/single_prediction/cat_or_dog_1.jpg\" width=20% alt=\"Dog\">\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Data/single_prediction/cat_or_dog_2.jpg\" width=20% alt=\"Cat\">\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "You are provided with a dataset consisting of 5,000 Cat images and 5,000 Dog images. We are going to train a Machine Learning model to learn differences between the two categories. The model will predict if a new unseen image is a Cat or Dog. The code architecture is robust and can be used to recognize any number of image categories, if provided with enough data.\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Convolution Neural Networks (CNN)\n",
    "\n",
    "<br></br>\n",
    "Convolution Neural Networks are good for pattern recognition and feature detection which is especially useful in image classification. Improve the performance of Convolution Neural Networks through hyper-parameter tuning, adding more convolution layers, adding more fully connected layers, or providing more correctly labeled data to the algorithm.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Create a Convolution Neural Network (CNN) with the following steps:\n",
    "\n",
    "1. Convolution\n",
    "2. Max Pooling\n",
    "3. Flattening\n",
    "4. Full Connection\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Check out [How to implement a neural network](http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/), also take a look at [A Friendly Introduction to Cross-Entropy Loss](http://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/).\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Convolution is a function derived from two other functions through an integration that expresses how the shape of one is modified by the other.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/01%20-%20Convolution%20Equation.png\" alt=\"Convolution-Equation\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "For image recognition, we convolve the input image with Feature Detectors (also known as Kernel or Filter) to generate a Feature Map (also known as Convolved Map or Activation Map). This reveals and preserves patterns in the image, and also compresses the image for easier processing. Feature Maps are generated by element-wise multiplication and addition of corresponding images with Filters consisting of multiple Feature Detectors. This allows the creation of multiple Feature Maps.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/02%20-%20CNN%20Example.png\" width=\"500\" alt=\"CNN-Example\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/03%20-%20CNN%20Feature%20Map.png\" width=\"500\" alt=\"CNN-Feature\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/04%20-%20CNN%20Multi%20Feature%20Map.png\" width=\"500\" alt=\"Feature-Map\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "This [Image Convolution Guide](https://docs.gimp.org/en/plug-in-convmatrix.html) allows you to play with various filters applied to an image. Edge Detect is a useful filters in Machine Learning. The algorithm creates filters that are not recognizable to humans, perhaps we learn with similar techniques in our subconscious. Feature Maps preserve spatial relationships between pixels throughout processing.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/05%20-%20Edge%20Detect%20Filter.png\" width=\"500\" alt=\"Edge-Detect\"></div>\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Rectified Linear Units (ReLU)\n",
    "\n",
    "<br></br>\n",
    "Rectifier Functions are applied to Convolution Neural Networks to increases non-linearity (breaks up linearity). This is an important step for image recognition with CNNs. Images are usually non-linear due to sharp transition of pixels, different colors, etc. ReLU functions help amplify the non-linearity of images so the ML model has an easier time finding patterns. \n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/06%20-%20ReLU%20Layer.png\" alt=\"ReLU\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "### Before ReLU\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/07%20-%20Before%20ReLU.png\" alt=\"Before-ReLU\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "###  After ReLU\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/08%20-%20After%20ReLU.png\" alt=\"After-ReLU\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "In the above example, the ReLU operation removed the Black Pixels so there's less White to Gray to Black transitions. Borders now have more abrupt Pixel changes. Microsoft argues that the using their Modified Rectifier Function works better for CNNs.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/09%20-%20Modified%20Rectifier.png\" alt=\"Rectifier\"></div>\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Max Pooling\n",
    "\n",
    "<br></br>\n",
    "Max Pooling finds the largest value of small grids in the Feature Map, this creates a Pooled Feature Map. Average Pooling (sub-sampling) takes the average values of small grids.  It makes sure that your Neural Network has Spatial Invariance (able to find learned features in new images that are slightly varied or distorted). Max Pooling provides resilience against shifter or rotated features. It also further distills Feature Maps (reduces size) while preserving spatial relationships of pixels. Removing unnecessary information also helps prevent overfitting. Read 'Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition.pdf'. Here is an online [CNN Visualization Tool](http://scs.ryerson.ca/~aharley/vis/conv/flat.html).\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/10%20-%20Max%20Pooling.png\" alt=\"Pooling\"></div>\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Flattening\n",
    "\n",
    "<br></br>\n",
    "Flattening puts values of the pooled Feature Map matrix into a 1-D vector. This makes it easy for the image data to pass through an Artificial Neural Network algorithm.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/11%20-%20Flattening.png\" width=\"400\" alt=\"Flattening\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/12%20-%20Flattening%202.png\" width=\"400\" alt=\"Flattening-2\"></div>\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Full Connection\n",
    "\n",
    "<br></br>\n",
    "This is when the output of a Convolution Neural Network is flattened and fed through a classic Artificial Neural Network. It's important to note that CNNs require fully-connected hidden layers where as regular ANNs don't necessarily need full connections.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/13%20-%20Full%20Connection.png\" width=\"400\" alt=\"Full-Connection\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "The process of CNN back-propagation adjusts weights of neurons, while adjusting Feature Maps.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/14%20-%20CNN%20Backprop.png\" width=\"400\" alt=\"Back-Propagation\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "When it's time for the CNN to make a decision between Cat or Dog, the final layer neurons 'vote' on probability of an image being a Cat or Dog (or any other categories you show it). The Neural Network adjusts votes according to the best weights it has determined through back-propagation.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/15%20-%20CNN%20Weighted%20Votes.png\" alt=\"Weighted-Votes\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Here is a summary of every step of a CNN, don't forget about the Rectifier Function that removes linearity in Feature Maps, also remember that the hidden layers are fully connected.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/16%20-%20CNN%20Full.png\" alt=\"CNN-Full\"></div>\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Pre-Processing (Images Augmentation)\n",
    "\n",
    "<br></br>\n",
    "This step modifies images to prevent over-fitting. This data augmentation trick can generate tons more data by applying random modifications to existing data like shearing, stretching, zooming, etc. This makes your dataset and algorithm more robust and generalized.\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Softmax and Cross-Entropy Cost Function\n",
    "\n",
    "<br></br>\n",
    "The Softmax function shown below is used to make sure that the probabilities of the output layer add up to one, this gives us a percentage guess. Watch this Geoffrey Hinton [video about the SoftMax Function](https://www.youtube.com/watch?v=mlaLLQofmR8).\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/18%20-%20Softmax%20Function.png\" alt=\"SoftMax\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "We had previously used the Mean Squared Error (MSE) Cost Function. For CNNs, it's better to use the Cross-Entropy Function as your Cost Function. We use Cross-Entropy as a Loss Function because it has a 'Log' term which helps amplify small Errors and better guide gradient descent.\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/17%20-%20Log%20Loss%20Function.png\" alt=\"Loss-Function\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/19%20-%20Cross%20Entropy%20Function.png\" width=\"200\" alt=\"Cross-Entropy\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/20%20-%20Cross%20Entropy%20Plug%20In.png\" width=\"400\" alt=\"Cross-Entropy-2\"></div>\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/AMoazeni/Machine-Learning-Image-Recognition/master/Jupyter%20Notebook/Images/21%20-%20Error%20Comparison.png\" alt=\"Error\"></div>\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "# Code\n",
    "\n",
    "<br></br>\n",
    "Download the code and run it with 'Jupyter Notebook' or copy the code into the 'Spyder' IDE found in the [Anaconda Distribution](https://www.anaconda.com/download/). 'Spyder' is similar to MATLAB, it allows you to step through the code and examine the 'Variable Explorer' to see exactly how the data is parsed and analyzed. Jupyter Notebook also offers a [Jupyter Variable Explorer Extension](http://volderette.de/jupyter-notebook-variable-explorer/) which is quite useful for keeping track of variables.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "```shell\n",
    "$ git clone https://github.com/AMoazeni/Machine-Learning-Image-Recognition.git\n",
    "$ cd Machine-Learning-Image-Recognition\n",
    "```\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 1999 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Convolution Neural Network\n",
    "# Part 1 - Building CNN Architecture and Import Data\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "# 'Sequential' library used to Initialize NN as sequence of layers (Alternative to Graph initialization)\n",
    "from keras.models import Sequential\n",
    "# 'Conv2D' for 1st step of adding convolution layers to images ('Conv3D' for videos with time as 3rd dimension)\n",
    "from keras.layers import Conv2D\n",
    "# 'MaxPooling2D' step 2 for pooling of max values from Convolution Layers\n",
    "from keras.layers import MaxPooling2D\n",
    "# 'Flatten' Pooled Layers for step 3\n",
    "from keras.layers import Flatten\n",
    "# 'Dense' for fully connected layers that feed into classic ANN\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initializing the CNN\n",
    "# Calling this object a 'classifier' because that's its job\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "# Apply a method 'add' on the object 'classifier'\n",
    "# Filter = Feature Detector = Feature Kernel\n",
    "# 'Conv2D' (Number of Filters, (Filter Row, Filter Column), input shape of inputs = (3 color channels, 64x64 -> 256x256 dimension of 2D array in each channel))\n",
    "# Start with 32 filters, work your way up to 64 -> 128 -> 256\n",
    "# 'input_shape' needs all picture inputs to be the same shape and format (2D array for B&W, 3D for Color images with each 2D array channel being Blue/Green/Red)\n",
    "# 'input_shape' parameter shape matters (3,64,64) vs (64,64,3)\n",
    "# 'Relu' Rectifier Activation Function used to get rid of -ve pixel values and increase non-linearity\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "# Reduces the size of the Feature Map by half (eg. 5x5 turns into 3x3 or 8x8 turns into 4x4)\n",
    "# Preserves Spatial Structure and performance of model while reducing computation time\n",
    "# 'pool_size' at least needs to be 2x2 to preserve Spatial Structure information (context around individual pixels)\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolution layer to improve performance\n",
    "# Only need 'input_shape' for Input Layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "# Take all the Pooled Feature Maps and put them into one huge single Vector that will input into a classic NN\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "# Add some fully connected hidden layers (start with a number of Node between input and output layers)\n",
    "# [Input Nodes(huge) - Output Nodes (2: Cat or Dog)] / 2 = ~128?...\n",
    "# 'Activation' function makes sure relevant Nodes get a stronger vote or no vote\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "# Add final Output Layer with binary options\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "# 'adam' Stochastic Gradient Descent optimizer\n",
    "# 'loss' function. Logarithmic loss for 2 categories use 'binary_crossentropy' and 'categorical_crossentropy' for more objects\n",
    "# 'metric' is the a performance metric\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Part 2 - Fitting the CNN to the images\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create random transformation from Data to increase Dataset and prevent overfitting\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# 'batch_size' is the number of images that go through the CNN every weight update cycle\n",
    "# Increase 'target_size' to improve model accuracy \n",
    "\n",
    "training_set = train_datagen.flow_from_directory('../Data/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('../Data/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n",
    "\n",
    "steps_per_epoch = training_set.samples // training_set.batch_size\n",
    "validation_steps = test_set.samples // test_set.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "250/250 [==============================] - 15s 62ms/step - loss: 0.1459 - accuracy: 0.9423 - val_loss: 0.6844 - val_accuracy: 0.8065\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 16s 65ms/step - loss: 0.1258 - accuracy: 0.9532 - val_loss: 0.7042 - val_accuracy: 0.8034\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 16s 64ms/step - loss: 0.1206 - accuracy: 0.9546 - val_loss: 0.7129 - val_accuracy: 0.8125\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 16s 66ms/step - loss: 0.1235 - accuracy: 0.9549 - val_loss: 0.7332 - val_accuracy: 0.7994\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 0.1301 - accuracy: 0.9511 - val_loss: 0.7001 - val_accuracy: 0.8054\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 15s 61ms/step - loss: 0.1070 - accuracy: 0.9603 - val_loss: 0.7288 - val_accuracy: 0.8120\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 18s 73ms/step - loss: 0.0968 - accuracy: 0.9630 - val_loss: 0.7676 - val_accuracy: 0.7984\n",
      "Epoch 8/20\n",
      "250/250 [==============================] - 17s 69ms/step - loss: 0.1064 - accuracy: 0.9614 - val_loss: 0.8382 - val_accuracy: 0.7918\n",
      "Epoch 9/20\n",
      "250/250 [==============================] - 17s 69ms/step - loss: 0.1013 - accuracy: 0.9625 - val_loss: 0.8762 - val_accuracy: 0.7933\n",
      "Epoch 10/20\n",
      "250/250 [==============================] - 19s 76ms/step - loss: 0.0928 - accuracy: 0.9660 - val_loss: 0.8483 - val_accuracy: 0.7928\n",
      "Epoch 11/20\n",
      "250/250 [==============================] - 17s 70ms/step - loss: 0.0879 - accuracy: 0.9669 - val_loss: 0.8850 - val_accuracy: 0.7928\n",
      "Epoch 12/20\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 0.0835 - accuracy: 0.9700 - val_loss: 0.8264 - val_accuracy: 0.8049\n",
      "Epoch 13/20\n",
      "250/250 [==============================] - 16s 66ms/step - loss: 0.0833 - accuracy: 0.9688 - val_loss: 0.9218 - val_accuracy: 0.7939\n",
      "Epoch 14/20\n",
      "250/250 [==============================] - 19s 74ms/step - loss: 0.0890 - accuracy: 0.9660 - val_loss: 0.8475 - val_accuracy: 0.7954\n",
      "Epoch 15/20\n",
      "250/250 [==============================] - 17s 68ms/step - loss: 0.0838 - accuracy: 0.9715 - val_loss: 0.8675 - val_accuracy: 0.8014\n",
      "Epoch 16/20\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 0.0759 - accuracy: 0.9736 - val_loss: 0.9389 - val_accuracy: 0.7843\n",
      "Epoch 17/20\n",
      "250/250 [==============================] - 18s 72ms/step - loss: 0.0859 - accuracy: 0.9664 - val_loss: 0.8511 - val_accuracy: 0.8054\n",
      "Epoch 18/20\n",
      "250/250 [==============================] - 18s 70ms/step - loss: 0.0881 - accuracy: 0.9676 - val_loss: 0.8398 - val_accuracy: 0.8014\n",
      "Epoch 19/20\n",
      "250/250 [==============================] - 17s 66ms/step - loss: 0.0783 - accuracy: 0.9704 - val_loss: 0.8687 - val_accuracy: 0.8044\n",
      "Epoch 20/20\n",
      "250/250 [==============================] - 17s 69ms/step - loss: 0.0756 - accuracy: 0.9737 - val_loss: 0.9067 - val_accuracy: 0.8039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x30d173f10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "# Increase 'epochs' to boost model performance (takes longer)\n",
    "classifier.fit(training_set,\n",
    "              steps_per_epoch = steps_per_epoch,\n",
    "              epochs = 20,\n",
    "              validation_data = test_set,\n",
    "              validation_steps = validation_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 31, 31, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 29, 29, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 14, 14, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               802944    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 813217 (3.10 MB)\n",
      "Trainable params: 813217 (3.10 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Save model to file\n",
    "# Architecture of the model, allowing to reuse trained models\n",
    "# Weights of the model\n",
    "# Training configuration (loss, optimizer)\n",
    "# State of the optimizer, allowing to resume training exactly where you left off\n",
    "classifier.save('../Data/saved_model/CNN_Cat_Dog_Model.h5')\n",
    "\n",
    "# Examine model\n",
    "classifier.summary()\n",
    "\n",
    "# Examine Weights\n",
    "classifier.weights\n",
    "\n",
    "# Examine Optimizer\n",
    "classifier.optimizer\n",
    "\n",
    "\n",
    "\n",
    "# Load saved Model\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('../Data/saved_model/CNN_Cat_Dog_Model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "The model class indices are: {'cats': 0, 'dogs': 1}\n",
      "\n",
      "Prediction: cat\n"
     ]
    }
   ],
   "source": [
    "# Part 3 - Making new predictions\n",
    "\n",
    "# Place a new picture of a cat or dog in 'single_prediction' folder and see if your model works\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('../Data/single_prediction/cat_or_dog_3.jpg', target_size = (64, 64))\n",
    "# Add a 3rd Color dimension to match Model expectation\n",
    "test_image = image.img_to_array(test_image)\n",
    "# Add one more dimension to beginning of image array so 'Predict' function can receive it (corresponds to Batch, even if only one batch)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "# We now need to pull up the mapping between 0/1 and cat/dog\n",
    "training_set.class_indices\n",
    "# Map is 2D so check the first row, first column value\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "# Print result\n",
    "\n",
    "print(\"The model class indices are:\", training_set.class_indices)\n",
    "\n",
    "print(\"\\nPrediction: \" + prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
